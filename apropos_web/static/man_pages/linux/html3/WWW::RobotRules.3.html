<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/>
<style>
table.head, table.foot { width: 100%; }
td.head-rtitle, td.foot-os { text-align: right; }
td.head-vol { text-align: center; }
table.foot td { width: 50%; }
table.head td { width: 33%; }
div.spacer { margin: 1em 0; }
</style>
<link rel="stylesheet" href="/static/man_pages/linux/style.css" type="text/css" media="all"/>
<title>
WWW::RobotRules(3pm)</title>
</head>
<body>
<div class="mandoc">
<table class="head">
<tbody>
<tr>
<td class="head-ltitle">
WWW::RobotRules(3pm)</td>
<td class="head-vol">
User Contributed Perl Documentation</td>
<td class="head-rtitle">
WWW::RobotRules(3pm)</td>
</tr>
</tbody>
</table>
<div class="section">
<h1>NAME</h1> WWW::RobotRules - database of robots.txt-derived permissions</div>
<div class="section">
<h1>SYNOPSIS</h1><br/>
 use WWW::RobotRules;<br/>
 my $rules = WWW::RobotRules-&gt;new('MOMspider/1.0');<br/>
<br/>
 use LWP::Simple qw(get);<br/>
<br/>
 {<br/>
   my $url = &quot;http://some.place/robots.txt&quot;;<br/>
   my $robots_txt = get $url;<br/>
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;<br/>
 }<br/>
<br/>
 {<br/>
   my $url = &quot;http://some.other.place/robots.txt&quot;;<br/>
   my $robots_txt = get $url;<br/>
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;<br/>
 }<br/>
<br/>
 # Now we can check if a URL is valid for those servers<br/>
 # whose &quot;robots.txt&quot; files we've gotten and parsed:<br/>
 if($rules-&gt;allowed($url)) {<br/>
     $c = get $url;<br/>
     ...<br/>
 }<br/>
</div>
<div class="section">
<h1>DESCRIPTION</h1> This module parses  <i>/robots.txt</i> files as specified in &quot;A Standard for Robot Exclusion&quot;, at &lt;http://www.robotstxt.org/wc/norobots.html&gt; Webmasters can use the  <i>/robots.txt</i> file to forbid conforming robots from accessing parts of their web site.<div class="spacer">
</div>
The parsed files are kept in a WWW::RobotRules object, and this object provides methods to check if access to a given URL is prohibited.  The same WWW::RobotRules object can be used for one or more parsed  <i>/robots.txt</i> files on any number of hosts.<div class="spacer">
</div>
The following methods are provided:<dl>
<dt>
$rules = WWW::RobotRules-&gt;new($robot_name)</dt>
<dd>
This is the constructor for WWW::RobotRules objects.  The first argument given to  <i>new()</i> is the name of the robot.</dd>
</dl>
<dl>
<dt>
$rules-&gt;parse($robot_txt_url, $content, $fresh_until)</dt>
<dd>
The <i>parse()</i> method takes as arguments the URL that was used to retrieve the  <i>/robots.txt</i> file, and the contents of the file.</dd>
</dl>
<dl>
<dt>
$rules-&gt;allowed($uri)</dt>
<dd>
Returns TRUE if this robot is allowed to retrieve this URL.</dd>
</dl>
<dl>
<dt>
$rules-&gt;agent([$name])</dt>
<dd>
Get/set the agent name. NOTE: Changing the agent name will clear the robots.txt rules and expire times out of the cache.</dd>
</dl>
</div>
<div class="section">
<h1>ROBOTS.TXT</h1> The format and semantics of the &quot;/robots.txt&quot; file are as follows (this is an edited abstract of &lt;http://www.robotstxt.org/wc/norobots.html&gt;):<div class="spacer">
</div>
The file consists of one or more records separated by one or more blank lines. Each record contains lines of the form<div class="spacer">
</div>
<br/>
  &lt;field-name&gt;: &lt;value&gt;<br/>
<div class="spacer">
</div>
The field name is case insensitive.  Text after the '#' character on a line is ignored during parsing.  This is used for comments.  The following &lt;field-names&gt; can be used:<dl>
<dt>
User-Agent</dt>
<dd>
The value of this field is the name of the robot the record is describing access policy for.  If more than one  <i>User-Agent</i> field is present the record describes an identical access policy for more than one robot. At least one field needs to be present per record.  If the value is '*', the record describes the default access policy for any robot that has not not matched any of the other records.<div style="height: 1.00em;">
&#160;</div>
The <i>User-Agent</i> fields must occur before the <i>Disallow</i> fields.  If a record contains a  <i>User-Agent</i> field after a <i>Disallow</i> field, that constitutes a malformed record.  This parser will assume that a blank line should have been placed before that  <i>User-Agent</i> field, and will break the record into two.  All the fields before the  <i>User-Agent</i> field will constitute a record, and the  <i>User-Agent</i> field will be the first field in a new record.</dd>
</dl>
<dl>
<dt>
Disallow</dt>
<dd>
The value of this field specifies a partial URL that is not to be visited. This can be a full path, or a partial path; any URL that starts with this value will not be retrieved</dd>
</dl>
<div class="spacer">
</div>
Unrecognized records are ignored.</div>
<div class="section">
<h1>ROBOTS.TXT EXAMPLES</h1> The following example &quot;/robots.txt&quot; file specifies that no robots should visit any URL starting with &quot;/cyberworld/map/&quot; or &quot;/tmp/&quot;:<div class="spacer">
</div>
<br/>
  User-agent: *<br/>
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space<br/>
  Disallow: /tmp/ # these will soon disappear<br/>
<div class="spacer">
</div>
This example &quot;/robots.txt&quot; file specifies that no robots should visit any URL starting with &quot;/cyberworld/map/&quot;, except the robot called &quot;cybermapper&quot;:<div class="spacer">
</div>
<br/>
  User-agent: *<br/>
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space<br/>
<br/>
  # Cybermapper knows where to go.<br/>
  User-agent: cybermapper<br/>
  Disallow:<br/>
<div class="spacer">
</div>
This example indicates that no robots should visit this site further:<div class="spacer">
</div>
<br/>
  # go away<br/>
  User-agent: *<br/>
  Disallow: /<br/>
<div class="spacer">
</div>
This is an example of a malformed robots.txt file.<div class="spacer">
</div>
<br/>
  # robots.txt for ancientcastle.example.com<br/>
  # I've locked myself away.<br/>
  User-agent: *<br/>
  Disallow: /<br/>
  # The castle is your home now, so you can go anywhere you like.<br/>
  User-agent: Belle<br/>
  Disallow: /west-wing/ # except the west wing!<br/>
  # It's good to be the Prince...<br/>
  User-agent: Beast<br/>
  Disallow:<br/>
<div class="spacer">
</div>
This file is missing the required blank lines between records. However, the intention is clear.</div>
<div class="section">
<h1>SEE ALSO</h1> LWP::RobotUA, WWW::RobotRules::AnyDBM_File</div>
<div class="section">
<h1>COPYRIGHT</h1><br/>
  Copyright 1995-2009, Gisle Aas<br/>
  Copyright 1995, Martijn Koster<br/>
<div class="spacer">
</div>
This library is free software; you can redistribute it and/or modify it under the same terms as Perl itself.</div>
<table class="foot">
<tr>
<td class="foot-date">
2011-03-13</td>
<td class="foot-os">
perl v5.10.1</td>
</tr>
</table>
</div>
</body>
</html>

